{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:20:50.736275Z",
     "start_time": "2023-07-01T14:20:50.724274Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/mnt/e/sign-lang-data-2\"\n",
    "# create a vocabulary with GPT2 tokenizer\n",
    "train = pd.read_csv(os.path.join(DATA_FOLDER, \"train.csv\"))\n",
    "train[\"phrase\"].to_csv(\"text.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:14:20.745662Z",
     "start_time": "2023-07-01T14:14:20.630555900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Créer le tokenizer BPE\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Pré-tokenizer qui sera utilisé pour découper votre texte en sous-parties, si nécessaire\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "# Décodeur qui permettra dµe reproduire le texte d'origine à partir des tokens\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Entraîner le tokenizer\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "tokenizer.train(files=[\"text.csv\"], trainer=trainer)  # Remplacez \"mon_fichier.txt\" par votre fichier texte\n",
    "\n",
    "# Enregistrer le tokenizer\n",
    "tokenizer.save(\"monBytePairTokenizer.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:15:39.036231700Z",
     "start_time": "2023-07-01T14:15:33.254983800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "with open(\"monBytePairTokenizer.json\", \"r\") as f:\n",
    "    js_file = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:21:42.743563600Z",
     "start_time": "2023-07-01T14:21:42.722529600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "vocab = js_file[\"model\"][\"vocab\"]\n",
    "merge = js_file[\"model\"][\"merges\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:22:50.081496500Z",
     "start_time": "2023-07-01T14:22:50.074986700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.BytePairTokenizer(vocab, merge)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:22:57.767004100Z",
     "start_time": "2023-07-01T14:22:57.531872300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(9,), dtype=int32, numpy=array([  -1,  477,  690,  369, 1653, 2002,  145, 3232,  156], dtype=int32)>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"The quick brown fox jumped\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:27:24.408069900Z",
     "start_time": "2023-07-01T14:27:23.229240900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[464, 2068, 7586, 21831, 11687, 13], [464, 21831, 21256, 13]]>\n",
      "tf.Tensor(b'The quick brown fox jumped.', shape=(), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 4,  5,  6, -1], dtype=int32)>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unbatched input.\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "tokenizer(\"The quick brown fox jumped.\")\n",
    "\n",
    "# Batched input.\n",
    "print(tokenizer([\"The quick brown fox jumped.\", \"The fox slept.\"]))\n",
    "\n",
    "# Detokenization.\n",
    "print(tokenizer.detokenize(tokenizer(\"The quick brown fox jumped.\")))\n",
    "\n",
    "\n",
    "# Custom vocabulary.\n",
    "vocab = {\"<|endoftext|>\": 0, \"a\": 4, \"Ġquick\": 5, \"Ġfox\": 6}\n",
    "merges = [\"Ġ q\", \"u i\", \"c k\", \"ui ck\", \"Ġq uick\"]\n",
    "merges += [\"Ġ f\", \"o x\", \"Ġf ox\"]\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer(vocabulary=vocab, merges=merges)\n",
    "tokenizer(\"a quick fox.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-01T14:04:57.343887100Z",
     "start_time": "2023-07-01T14:04:53.912016400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
