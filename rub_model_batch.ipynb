{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:41.010028800Z",
     "start_time": "2023-05-09T05:15:37.501397400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 07:15:38.242044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 07:15:39.238837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 15 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import utils, optimizers, losses, metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=15, progress_bar=True)\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from tqdm.keras import TqdmCallback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "tqdm.pandas()\n",
    "# keras set seed\n",
    "utils.set_random_seed(1234)\n",
    "# Load the data\n",
    "DATA_FOLDER = \"/mnt/e/sign-lang-data/\"\n",
    "train = pd.read_csv(os.path.join(DATA_FOLDER, \"train_processed.csv\"))\n",
    "dict_sign = {}\n",
    "for i, sign in enumerate(train[\"sign\"].unique()):\n",
    "    dict_sign[sign] = i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:41.365398700Z",
     "start_time": "2023-05-09T05:15:41.012533200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#train = train.sample(10000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:41.368908800Z",
     "start_time": "2023-05-09T05:15:41.366394200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 07:15:41.619434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:41.805764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:41.805817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:41.807544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:41.807585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:41.807610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:43.004112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:43.004181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:43.004190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-09 07:15:43.004226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2d:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 07:15:43.004254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7335 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:2d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from custom_model.first import CustomModel\n",
    "batch_size = 64\n",
    "timesteps = 100\n",
    "features = 1629\n",
    "nb_classes = len(train[\"sign\"].unique())\n",
    "\n",
    "model = CustomModel(batch_size, timesteps, features, nb_classes)\n",
    "# model = CustomModel( input_layer, output_layer)\n",
    "#model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['categorical_accuracy'], sample_weight_mode='temporal')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:43.677905800Z",
     "start_time": "2023-05-09T05:15:41.368908800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_label(sign, dict_):\n",
    "    return utils.to_categorical(dict_[sign], num_classes=len(train[\"sign\"].unique()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:43.683905200Z",
     "start_time": "2023-05-09T05:15:43.680907300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/94477 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18a23ef8d2704127b0f16b017fcd4128"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"new_path\"] = train[\"new_path\"].progress_apply(lambda x: os.path.join(DATA_FOLDER, x))\n",
    "#train[\"label\"] = train[\"sign\"].parallel_apply(lambda x: get_label(x, dict_sign))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:43.782197200Z",
     "start_time": "2023-05-09T05:15:43.683905200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X = train[\"new_path\"].values\n",
    "Y = train[\"sign\"].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:43.786196Z",
     "start_time": "2023-05-09T05:15:43.784197100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# split the dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.164068100Z",
     "start_time": "2023-05-09T05:15:43.786196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.191179Z",
     "start_time": "2023-05-09T05:15:44.180931300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.197852Z",
     "start_time": "2023-05-09T05:15:44.189122900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def load_npy_file(filename, label):\n",
    "    # Load the NPY file\n",
    "    max_nb_frames = timesteps\n",
    "    import numpy as np\n",
    "\n",
    "    filename = filename.numpy().decode(\"utf-8\")\n",
    "    label = label.numpy().decode(\"utf-8\")\n",
    "\n",
    "    sequence = np.load(filename).astype(np.float32)\n",
    "\n",
    "    sequence = np.reshape(sequence, (sequence.shape[0],1629))\n",
    "\n",
    "    if len(sequence) > max_nb_frames:\n",
    "        sequence = sequence[:max_nb_frames]\n",
    "    else:\n",
    "        sequence = np.concatenate((sequence, np.zeros((max_nb_frames - len(sequence), 1629))))\n",
    "    label = utils.to_categorical(dict_sign[label], num_classes=250)\n",
    "    return sequence, label\n",
    "\n",
    "\n",
    "def load_parquet_file(filename, label):\n",
    "    max_nb_frames = timesteps\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    filename = filename.numpy().decode(\"utf-8\")\n",
    "    label = label.numpy().decode(\"utf-8\")\n",
    "\n",
    "    df = pd.read_parquet(filename, engine=\"pyarrow\", columns=[\"frame\", \"x\", \"y\", \"z\"])\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    #npy_data = np.load(filename.numpy()).astype(np.float32)\n",
    "    # Return the data and label as a tuple\n",
    "    # convert label custom_model numpy array\n",
    "    #convert label (b'[0,1]') to [0,1]\n",
    "\n",
    "    nbframes = len(df[\"frame\"].unique())\n",
    "    frames = np.zeros((nbframes, 543, 3))\n",
    "    for step ,(name, timestep) in enumerate(df.groupby(\"frame\")):\n",
    "        frames[step, :, :] = timestep[[\"x\", \"y\", \"z\"]].values\n",
    "\n",
    "    sequence = np.reshape(np.stack(frames), (len(frames),1629))\n",
    "\n",
    "    if len(sequence) > max_nb_frames:\n",
    "        sequence = sequence[:max_nb_frames]\n",
    "    else:\n",
    "        sequence = np.concatenate((sequence, np.zeros((max_nb_frames - len(sequence), 1629))))\n",
    "    label = utils.to_categorical(dict_sign[label], num_classes=250)\n",
    "    return sequence, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.197852Z",
     "start_time": "2023-05-09T05:15:44.195808600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train[\"path\"].shape[0])\n",
    "\n",
    "# Load the NPY files and labels in batches\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda filename, label: tf.py_function(\n",
    "    load_npy_file, [filename, label], [tf.float32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "# Prefetch the data for improved performance\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.238920200Z",
     "start_time": "2023-05-09T05:15:44.198853200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "val_dataset = val_dataset.shuffle(buffer_size=train[\"path\"].shape[0])\n",
    "\n",
    "# Load the NPY files and labels in batches\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda filename, label: tf.py_function(\n",
    "    load_npy_file, [filename, label], [tf.float32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "# Prefetch the data for improved performance\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.247426800Z",
     "start_time": "2023-05-09T05:15:44.238920200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adamax() #optimizers.SGD(learning_rate=1e-5, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "loss_fn = losses.CategoricalCrossentropy()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = metrics.CategoricalAccuracy()\n",
    "val_acc_metric = metrics.CategoricalAccuracy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.268935700Z",
     "start_time": "2023-05-09T05:15:44.251426600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2\n",
    "@tf.function\n",
    "def train_step(model,x_batch_train, y_batch_train, optimizer, loss_fn, train_acc_metric, max_sequence_length=150):\n",
    "\n",
    "    for seq in range(timesteps // max_sequence_length):\n",
    "       with tf.GradientTape() as tape:\n",
    "           logits = model(x_batch_train[:, seq:seq + max_sequence_length], training=True)\n",
    "           loss_value = loss_fn(y_batch_train[:, :], logits)\n",
    "\n",
    "       grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "       optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    model.reset_states()\n",
    "# Update training metric.\n",
    "    train_acc_metric.update_state(y_batch_train[:, :], logits)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x_batch_train, y_batch_train, loss_fn, train_acc_metric, max_sequence_length=150):\n",
    "\n",
    "    for seq in range(timesteps // max_sequence_length):\n",
    "       logits = model(x_batch_train[:, seq:seq + max_sequence_length], training=True)\n",
    "       loss_value = loss_fn(y_batch_train[:, :], logits)\n",
    "    model.reset_states()\n",
    "# Update training metric.\n",
    "    train_acc_metric.update_state(y_batch_train[:, :], logits)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "def custom_fit(model, epochs, train_dataset, val_dataset=None):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        max_sequence_length = timesteps\n",
    "        # Iterate over the batches of the dataset.\n",
    "        tqdm_bar = tqdm(train_dataset)\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(tqdm_bar):\n",
    "            if x_batch_train.shape[0] != batch_size:\n",
    "                continue\n",
    "                #x_batch_train = tf.concat([x_batch_train, tf.zeros((batch_size - x_batch_train.shape[0], max_sequence_length, 1629))], axis=0)\n",
    "\n",
    "            loss_value = train_step(model,x_batch_train, y_batch_train, optimizer, loss_fn, train_acc_metric, max_sequence_length)\n",
    "\n",
    "            train_acc = train_acc_metric.result()\n",
    "            # Display metrics at the end of each 10 batch.\n",
    "            if step % 10 == 0:\n",
    "                tqdm_bar.set_postfix(train_loss=loss_value.numpy(), train_acc=float(train_acc))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        if val_dataset is not None:\n",
    "            tqdm_bar = tqdm(val_dataset)\n",
    "            for step, (x_batch_val, y_batch_val) in enumerate(tqdm_bar):\n",
    "                if x_batch_val.shape[0] != batch_size:\n",
    "                    continue\n",
    "\n",
    "                val_loss = test_step(model, x_batch_val, y_batch_val, loss_fn, val_acc_metric, max_sequence_length)\n",
    "                if step % 10 == 0:\n",
    "                    tqdm_bar.set_postfix(val_loss=val_loss.numpy(), val_acc=float(val_acc_metric.result()))\n",
    "            val_acc_metric.reset_states()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:15:44.268935700Z",
     "start_time": "2023-05-09T05:15:44.262428100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_fit(model, 1000, train_dataset, val_dataset=val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
